{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Multiphase Model Training\n",
    "\n",
    "Note: Make sure to run the notebook in virtualenv. \n",
    "\n",
    "## Loading the Data\n",
    "The code below loads data and labels from `/research/rih-cs/datasets/elvo-multiphase`.\n",
    "\n",
    "Each phase data is stored under `/research/rih-cs/datasets/elvo-multiphase/preprocessed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  logging\n",
    "\n",
    "def configure_logger():\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test / Val Split\n",
    "We will iterate through the pos and neg directory of phase1 to get the index of our train/test/val set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = ['P25', 'P48', 'P62', 'P72', 'P144', 'P149', 'P1', 'P4', 'P16', 'P21', 'P32', \\\n",
    "'P36', 'P38', 'P52', 'P59', 'P88', 'P89', 'P118', 'P164', 'P232', 'P255', 'P266', 'P280', \\\n",
    "'P289', 'P73', 'P78', 'P120', 'P142', 'P126', 'P145', 'P147', 'P3', 'P5', 'P6', 'P15', \\\n",
    "'P17', 'P22', 'P28', 'P29', 'P34', 'P57', 'P58', 'P61', 'P66', 'P68', 'P70', 'P77', 'P80', \\\n",
    "'P85', 'P87', 'P94', 'P102', 'P106', 'P107', 'P110', 'P125', 'P127', 'P130', 'P134', 'P135', \\\n",
    "'P141', 'P150', 'P152', 'P153', 'P158', 'P163', 'P166', 'P179', 'P180', 'P181', 'P182', 'P185', \\\n",
    "'P207', 'P209', 'P210', 'P216', 'P218', 'P222', 'P224', 'P225', 'P231', 'P8', 'P13', 'P18', \\\n",
    "'P24', 'P33', 'P40', 'P43', 'P44', 'P47', 'P51', 'P53', 'P56', 'P63', 'P67', 'P69', 'P81', \\\n",
    "'P100', 'P101', 'P111', 'P117', 'P124', 'P146', 'P168', 'P184', 'P187', 'P188', 'P208', 'P212', \\\n",
    "'P248', 'P112', 'P2', 'P10', 'P20', 'P26', 'P46', 'P60', 'P79', 'P93', 'P95', 'P98', 'P116', 'P121', \\\n",
    "'P136', 'P143', 'P148', 'P160', 'P189', 'P203', 'P71', 'P97', 'P140', 'P84', 'P92', 'P131', 'P7', \\\n",
    "'P42', 'P129', 'P137', 'P154', 'P159', 'P176', 'P201', 'P213', 'P9', 'P11', 'P12', 'P19', 'P23', \\\n",
    "'P27', 'P30', 'P31', 'P35', 'P39', 'P45', 'P54', 'P55', 'P64', 'P65', 'P74', 'P91', 'P96', 'P99', \\\n",
    "'P104', 'P105', 'P108', 'P109', 'P113', 'P114', 'P119', 'P122', 'P123', 'P128', 'P132', 'P133', 'P139', \\\n",
    "'P151', 'P155', 'P156', 'P157', 'P165', 'P169', 'P173', 'P174', 'P177', 'P183', 'P186', 'P190', 'P192', \\\n",
    "'P193', 'P194', 'P197', 'P199', 'P200', 'P202', 'P205', 'P14', 'P41', 'P49', 'P75', 'P83', 'P86', 'P90', \\\n",
    "'P103', 'P167', 'P171', 'P196', 'P198', 'P204', 'P214', 'P254', 'P191'] \n",
    "\n",
    "TEST_DATA = ['P252', 'P265', 'P162', 'P170', 'P172', 'P178', 'P195', 'P221', 'P253', 'P234', 'P236', 'P237', \\\n",
    "'P241', 'P262', 'P272', 'P277', 'P282', 'P284', 'P285', 'P288', 'P291', 'P293', 'P296', 'P220', 'P228', \\\n",
    "'P246', 'P250', 'P270', 'P273', 'P283', 'P302', 'P268', 'P292', 'P226', 'P245', 'P263', 'P269', 'P286', \\\n",
    "'P217', 'P219', 'P233', 'P244', 'P206', 'P211', 'P215', 'P223', 'P227', 'P235', 'P243', 'P257', 'P258', \\\n",
    "'P260', 'P261', 'P267', 'P275', 'P278', 'P264', 'P274', 'P276', 'P279', 'P242']\n",
    "\n",
    "VAL_DATA = ['P271', 'P259', 'P238', 'P281', 'P229', 'P240', 'P297', 'P309', 'P310', 'P50', 'P76', 'P230', \\\n",
    "'P304', 'P305', 'P306', 'P307', 'P308', 'P300', 'P290', 'P298', 'P299', 'P249', 'P239', 'P294', 'P301', \\\n",
    "'P303', 'P161', 'P256', 'P37', 'P287', 'P295', 'P82', 'P247'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/research/rih-cs/datasets/elvo-multiphase/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LENGTH, WIDTH, HEIGHT = (3, 230, 230)\n",
    "TRAIN_INDICES = []\n",
    "TEST_INDICES = []\n",
    "VAL_INDICES = []\n",
    "\n",
    "# Usage: np.stack(train_arrays)\n",
    "train_arrays = []\n",
    "test_arrays = []\n",
    "val_arrays = []\n",
    "\n",
    "def load_training_data(): \n",
    "    \"\"\"\n",
    "    Returns 4D matrix of training data\n",
    "    Data is in the form (n_samples, 1, w, h). \n",
    "    Samples are sorted respectively according to the specs in TRAIN_DATA, TEST_DATA, VAL_DATA\n",
    "    \"\"\"\n",
    "\n",
    "    phase1_pos_files = sorted(os.listdir(data_path + 'phase1/pos/'))\n",
    "    for i, filename in enumerate(phase1_pos_files):\n",
    "        arr = np.load(data_path + 'phase1/pos/' + filename)\n",
    "        matching_name = os.path.splitext(filename)[0] \n",
    "        if matching_name in TRAIN_DATA:\n",
    "            train_arrays.append(arr)\n",
    "            TRAIN_INDICES.append(i)\n",
    "        elif matching_name in TEST_DATA: \n",
    "            test_arrays.append(arr)\n",
    "            TEST_INDICES.append(i)\n",
    "        elif matching_name in VAL_DATA: \n",
    "            val_arrays.append(arr)\n",
    "            VAL_INDICES.append(i)\n",
    "        else: \n",
    "            logging.info(\n",
    "            f'training file {filename}, {matching_name} is not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_data()\n",
    "# 12/02/2018 has 406 positive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEST_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VAL_INDICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing into one input for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the multiple (three) parallel phases as input for the LSTM model \n",
    "# Doc: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "\n",
    "# positive i = 0 - 94; negative i = 95 - 172 \n",
    "lstm_input = np.zeros((162, 3, 3 * 230 * 230))\n",
    "\n",
    "phase1_full_path = data_path + 'phase1/pos/'\n",
    "phase2_full_path = data_path + 'phase2/pos/'\n",
    "phase3_full_path = data_path + 'phase3/pos/'\n",
    "\n",
    "# There are 96 files in each directory. 0-66 (training); 67-86 (testing); 87-95 (validation)\n",
    "neg_phase1_full_path = data_path + 'phase1/neg/'\n",
    "neg_phase2_full_path = data_path + 'phase2/neg/'\n",
    "neg_phase3_full_path = data_path + 'phase3/neg/'\n",
    "\n",
    "def create_lstm_training_input(): \n",
    "    phase1_pos_files = sorted(os.listdir(phase1_full_path))\n",
    "    phase2_pos_files = sorted(os.listdir(phase2_full_path))\n",
    "    phase3_pos_files = sorted(os.listdir(phase3_full_path))\n",
    "    \n",
    "    i = 0 \n",
    "    # TRAIN_INDICES is selected in the google spreadsheet based on the data's location\n",
    "    for index in TRAIN_INDICES: \n",
    "        phase1_arr = np.load(phase1_full_path + phase1_pos_files[index])\n",
    "        phase2_arr = np.load(phase2_full_path + phase2_pos_files[index])\n",
    "        phase3_arr = np.load(phase3_full_path + phase3_pos_files[index])\n",
    "        \n",
    "        # TODO: check if the resize array is good \n",
    "        re_phase1_arr = np.resize(phase1_arr, (3, 230, 230))\n",
    "        re_phase2_arr = np.resize(phase2_arr, (3, 230, 230))\n",
    "        re_phase3_arr = np.resize(phase3_arr, (3, 230, 230))\n",
    "        \n",
    "        re_phase1_arr = re_phase1_arr.reshape(3 * 230 * 230)\n",
    "        re_phase2_arr = re_phase2_arr.reshape(3 * 230 * 230)\n",
    "        re_phase3_arr = re_phase3_arr.reshape(3 * 230 * 230)\n",
    "        \n",
    "        lstm_input[i] = np.array([re_phase1_arr,re_phase2_arr,re_phase3_arr])\n",
    "        i += 1     \n",
    "    \n",
    "    phase1_neg_files = sorted(os.listdir(neg_phase1_full_path))\n",
    "    phase2_neg_files = sorted(os.listdir(neg_phase2_full_path))\n",
    "    phase3_neg_files = sorted(os.listdir(neg_phase3_full_path))\n",
    "    \n",
    "    # As illustrated above, the first 66 negative data would be in the training set (0-66) \n",
    "    for neg_index in range(67): \n",
    "        phase1_arr = np.load(neg_phase1_full_path + phase1_neg_files[neg_index])\n",
    "        phase2_arr = np.load(neg_phase2_full_path + phase2_neg_files[neg_index])\n",
    "        phase3_arr = np.load(neg_phase3_full_path + phase3_neg_files[neg_index])\n",
    "        \n",
    "        re_phase1_arr = np.resize(phase1_arr, (3, 230, 230))\n",
    "        re_phase2_arr = np.resize(phase2_arr, (3, 230, 230))\n",
    "        re_phase3_arr = np.resize(phase3_arr, (3, 230, 230))\n",
    "        \n",
    "        re_phase1_arr = re_phase1_arr.reshape(3 * 230 * 230)\n",
    "        re_phase2_arr = re_phase2_arr.reshape(3 * 230 * 230)\n",
    "        re_phase3_arr = re_phase3_arr.reshape(3 * 230 * 230)\n",
    "        \n",
    "        lstm_input[i] = np.array([re_phase1_arr,re_phase2_arr,re_phase3_arr])\n",
    "        i += 1     \n",
    "        \n",
    "    print(i)\n",
    "    return lstm_input\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM input for validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the multiple (three) parallel phases as input for the LSTM model \n",
    "# Doc: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "\n",
    "# positive i = 0-21; negative i = 87-95\n",
    "lstm_val_input = np.zeros((31, 3, 3 * 230 * 230))\n",
    "\n",
    "phase1_full_path = data_path + 'phase1/pos/'\n",
    "phase2_full_path = data_path + 'phase2/pos/'\n",
    "phase3_full_path = data_path + 'phase3/pos/'\n",
    "\n",
    "# There are 96 files in each directory. 0-66 (training); 67-86 (testing); 87-95 (validation)\n",
    "neg_phase1_full_path = data_path + 'phase1/neg/'\n",
    "neg_phase2_full_path = data_path + 'phase2/neg/'\n",
    "neg_phase3_full_path = data_path + 'phase3/neg/'\n",
    "\n",
    "def create_lstm_val_input(): \n",
    "    phase1_pos_files = sorted(os.listdir(phase1_full_path))\n",
    "    phase2_pos_files = sorted(os.listdir(phase2_full_path))\n",
    "    phase3_pos_files = sorted(os.listdir(phase3_full_path))\n",
    "    \n",
    "    j = 0 \n",
    "    # TRAIN_INDICES is selected in the google spreadsheet based on the data's location\n",
    "    for index in VAL_INDICES: \n",
    "        phase1_arr = np.load(phase1_full_path + phase1_pos_files[index])\n",
    "        phase2_arr = np.load(phase2_full_path + phase2_pos_files[index])\n",
    "        phase3_arr = np.load(phase3_full_path + phase3_pos_files[index])\n",
    "        \n",
    "        re_phase1_arr = np.resize(phase1_arr, (3, 230, 230))\n",
    "        re_phase2_arr = np.resize(phase2_arr, (3, 230, 230))\n",
    "        re_phase3_arr = np.resize(phase3_arr, (3, 230, 230))\n",
    "        \n",
    "        re_phase1_arr = re_phase1_arr.reshape(3 * 230 * 230)\n",
    "        re_phase2_arr = re_phase2_arr.reshape(3 * 230 * 230)\n",
    "        re_phase3_arr = re_phase3_arr.reshape(3 * 230 * 230)\n",
    "        \n",
    "        lstm_val_input[j] = np.array([re_phase1_arr,re_phase2_arr,re_phase3_arr])\n",
    "        j += 1     \n",
    "    \n",
    "    phase1_neg_files = sorted(os.listdir(neg_phase1_full_path))\n",
    "    phase2_neg_files = sorted(os.listdir(neg_phase2_full_path))\n",
    "    phase3_neg_files = sorted(os.listdir(neg_phase3_full_path))\n",
    "    \n",
    "    # As illustrated above, the last 8 would be validation dataset \n",
    "    for neg_index in range(87, 96): \n",
    "        phase1_arr = np.load(neg_phase1_full_path + phase1_neg_files[neg_index])\n",
    "        phase2_arr = np.load(neg_phase2_full_path + phase2_neg_files[neg_index])\n",
    "        phase3_arr = np.load(neg_phase3_full_path + phase3_neg_files[neg_index])\n",
    "        \n",
    "        re_phase1_arr = np.resize(phase1_arr, (3, 230, 230))\n",
    "        re_phase2_arr = np.resize(phase2_arr, (3, 230, 230))\n",
    "        re_phase3_arr = np.resize(phase3_arr, (3, 230, 230))\n",
    "        \n",
    "        re_phase1_arr = re_phase1_arr.reshape(3 * 230 * 230)\n",
    "        re_phase2_arr = re_phase2_arr.reshape(3 * 230 * 230)\n",
    "        re_phase3_arr = re_phase3_arr.reshape(3 * 230 * 230)\n",
    "        \n",
    "        lstm_val_input[j] = np.array([re_phase1_arr,re_phase2_arr,re_phase3_arr])\n",
    "        j += 1     \n",
    "        \n",
    "    print(j)\n",
    "    return lstm_val_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "lstm_training_input = create_lstm_training_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -990.,  -988.,  -987., ...,  -920.,  -864.,  -874.],\n",
       "       [-1024., -1024., -1024., ..., -1010., -1008., -1000.],\n",
       "       [-1024., -1024., -1024., ..., -1011., -1007., -1010.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_training_input[161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = lstm_training_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "lstm_val_input = create_lstm_val_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, BatchNormalization, Dense, Flatten, Embedding\n",
    "from keras.layers.recurrent import RNN, LSTM \n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 3, 158700)\n",
      "Train on 162 samples, validate on 31 samples\n",
      "Epoch 1/10\n",
      "162/162 [==============================] - 5s 28ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 2/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 3/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 4/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 5/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 6/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 7/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 8/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 9/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n",
      "Epoch 10/10\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.5935 - acc: 0.5864 - val_loss: 4.6284 - val_acc: 0.7097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd137a41ef0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "print(lstm_input.shape)\n",
    "num_samples = lstm_input.shape[0]\n",
    "num_steps = lstm_input.shape[1]\n",
    "num_features = lstm_input.shape[2]\n",
    "num_classes = 2\n",
    "pos_y = np.ones((95,))\n",
    "neg_y = np.zeros((67,))\n",
    "y_train = np.concatenate((pos_y,neg_y))\n",
    "x_train = lstm_input\n",
    "val_pos_y = np.ones((22,))\n",
    "val_neg_y = np.zeros((9,))\n",
    "y_val = np.concatenate((val_pos_y,val_neg_y))\n",
    "x_val = lstm_val_input\n",
    "model.add(LSTM(32, input_shape=(num_steps, num_features)))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=18, epochs=10, \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 11.3797 - acc: 0.0830 - val_loss: 11.4148 - val_acc: 0.0600\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 304us/step - loss: 11.3704 - acc: 0.0950 - val_loss: 11.4173 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 312us/step - loss: 11.3687 - acc: 0.1000 - val_loss: 11.4115 - val_acc: 0.0600\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 11.3685 - acc: 0.0930 - val_loss: 11.4130 - val_acc: 0.0500\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 409us/step - loss: 11.3672 - acc: 0.0970 - val_loss: 11.4157 - val_acc: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8bd850bda0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playground: Example from https://keras.io/getting-started/sequential-model-guide/\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,# return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
