{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Multiphase Model Training\n",
    "\n",
    "Note: Make sure to run the notebook in virtualenv. \n",
    "\n",
    "## Loading the Data\n",
    "The code below loads data and labels from `/research/rih-cs/datasets/elvo-multiphase`.\n",
    "\n",
    "Each phase data is stored under `/research/rih-cs/datasets/elvo-multiphase/preprocessed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  logging\n",
    "\n",
    "def configure_logger():\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test / Val Split\n",
    "We will iterate through the pos and neg directory of phase1 to get the index of our train/test/val set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = ['P25', 'P48', 'P62', 'P72', 'P144', 'P149', 'P1', 'P4', 'P16', 'P21', 'P32', \\\n",
    "'P36', 'P38', 'P52', 'P59', 'P88', 'P89', 'P118', 'P164', 'P232', 'P255', 'P266', 'P280', \\\n",
    "'P289', 'P73', 'P78', 'P120', 'P142', 'P126', 'P145', 'P147', 'P3', 'P5', 'P6', 'P15', \\\n",
    "'P17', 'P22', 'P28', 'P29', 'P34', 'P57', 'P58', 'P61', 'P66', 'P68', 'P70', 'P77', 'P80', \\\n",
    "'P85', 'P87', 'P94', 'P102', 'P106', 'P107', 'P110', 'P125', 'P127', 'P130', 'P134', 'P135', \\\n",
    "'P141', 'P150', 'P152', 'P153', 'P158', 'P163', 'P166', 'P179', 'P180', 'P181', 'P182', 'P185', \\\n",
    "'P207', 'P209', 'P210', 'P216', 'P218', 'P222', 'P224', 'P225', 'P231', 'P8', 'P13', 'P18', \\\n",
    "'P24', 'P33', 'P40', 'P43', 'P44', 'P47', 'P51', 'P53', 'P56', 'P63', 'P67', 'P69', 'P81', \\\n",
    "'P100', 'P101', 'P111', 'P117', 'P124', 'P146', 'P168', 'P184', 'P187', 'P188', 'P208', 'P212', \\\n",
    "'P248', 'P112', 'P2', 'P10', 'P20', 'P26', 'P46', 'P60', 'P79', 'P93', 'P95', 'P98', 'P116', 'P121', \\\n",
    "'P136', 'P143', 'P148', 'P160', 'P189', 'P203', 'P71', 'P97', 'P140', 'P84', 'P92', 'P131', 'P7', \\\n",
    "'P42', 'P129', 'P137', 'P154', 'P159', 'P176', 'P201', 'P213', 'P9', 'P11', 'P12', 'P19', 'P23', \\\n",
    "'P27', 'P30', 'P31', 'P35', 'P39', 'P45', 'P54', 'P55', 'P64', 'P65', 'P74', 'P91', 'P96', 'P99', \\\n",
    "'P104', 'P105', 'P108', 'P109', 'P113', 'P114', 'P119', 'P122', 'P123', 'P128', 'P132', 'P133', 'P139', \\\n",
    "'P151', 'P155', 'P156', 'P157', 'P165', 'P169', 'P173', 'P174', 'P177', 'P183', 'P186', 'P190', 'P192', \\\n",
    "'P193', 'P194', 'P197', 'P199', 'P200', 'P202', 'P205', 'P14', 'P41', 'P49', 'P75', 'P83', 'P86', 'P90', \\\n",
    "'P103', 'P167', 'P171', 'P196', 'P198', 'P204', 'P214', 'P254', 'P191'] \n",
    "\n",
    "TEST_DATA = ['P252', 'P265', 'P162', 'P170', 'P172', 'P178', 'P195', 'P221', 'P253', 'P234', 'P236', 'P237', \\\n",
    "'P241', 'P262', 'P272', 'P277', 'P282', 'P284', 'P285', 'P288', 'P291', 'P293', 'P296', 'P220', 'P228', \\\n",
    "'P246', 'P250', 'P270', 'P273', 'P283', 'P302', 'P268', 'P292', 'P226', 'P245', 'P263', 'P269', 'P286', \\\n",
    "'P217', 'P219', 'P233', 'P244', 'P206', 'P211', 'P215', 'P223', 'P227', 'P235', 'P243', 'P257', 'P258', \\\n",
    "'P260', 'P261', 'P267', 'P275', 'P278', 'P264', 'P274', 'P276', 'P279', 'P242']\n",
    "\n",
    "VAL_DATA = ['P271', 'P259', 'P238', 'P281', 'P229', 'P240', 'P297', 'P309', 'P310', 'P50', 'P76', 'P230', \\\n",
    "'P304', 'P305', 'P306', 'P307', 'P308', 'P300', 'P290', 'P298', 'P299', 'P249', 'P239', 'P294', 'P301', \\\n",
    "'P303', 'P161', 'P256', 'P37', 'P287', 'P295', 'P82', 'P247'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/research/rih-cs/datasets/elvo-multiphase/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH, WIDTH, HEIGHT = (3, 230, 230)\n",
    "TRAIN_INDICES = []\n",
    "TEST_INDICES = []\n",
    "VAL_INDICES = []\n",
    "\n",
    "# Usage: np.stack(train_arrays)\n",
    "train_arrays = []\n",
    "test_arrays = []\n",
    "val_arrays = []\n",
    "\n",
    "def load_training_data(): \n",
    "    \"\"\"\n",
    "    Returns 4D matrix of training data\n",
    "    Data is in the form (n_samples, 1, w, h). \n",
    "    Samples are sorted respectively according to the specs in TRAIN_DATA, TEST_DATA, VAL_DATA\n",
    "    \"\"\"\n",
    "\n",
    "    phase1_pos_files = sorted(os.listdir(data_path + 'phase1/pos/'))\n",
    "    for i, filename in enumerate(phase1_pos_files):\n",
    "        arr = np.load(data_path + 'phase1/pos/' + filename)\n",
    "        if arr.shape == (LENGTH, WIDTH, HEIGHT):\n",
    "            matching_name = os.path.splitext(filename)[0] \n",
    "            if matching_name in TRAIN_DATA:\n",
    "                train_arrays.append(arr)\n",
    "                TRAIN_INDICES.append(i)\n",
    "            elif matching_name in TEST_DATA: \n",
    "                test_arrays.append(arr)\n",
    "                TEST_INDICES.append(i)\n",
    "            elif matching_name in VAL_DATA: \n",
    "                val_arrays.append(arr)\n",
    "                VAL_INDICES.append(i)\n",
    "            else: \n",
    "                logging.info(\n",
    "                f'training file {filename}, {matching_name} is not found.')\n",
    "        else:\n",
    "            logging.info(\n",
    "                f'training file {filename} has incorrect shape {arr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_data()\n",
    "# 12/02/2018 has 406 positive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 27,\n",
       " 29,\n",
       " 32,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 59,\n",
       " 60,\n",
       " 62,\n",
       " 65,\n",
       " 82,\n",
       " 99,\n",
       " 100,\n",
       " 114,\n",
       " 116,\n",
       " 117,\n",
       " 120,\n",
       " 121,\n",
       " 124,\n",
       " 125,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 27,\n",
       " 29,\n",
       " 32,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 59,\n",
       " 60,\n",
       " 62,\n",
       " 65,\n",
       " 82,\n",
       " 99,\n",
       " 100,\n",
       " 114,\n",
       " 116,\n",
       " 117,\n",
       " 120,\n",
       " 121,\n",
       " 124,\n",
       " 125,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the indices \n",
    "TRAIN_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 3, 230, 230)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of train array output\n",
    "train_shape = np.stack(train_arrays).shape\n",
    "n_train = train_shape[0]\n",
    "train_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing into one input for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  5,  6, 15, 16],\n",
       "       [ 3,  4,  7,  8, 17, 18]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How would the data after concatenation look like\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "c = np.array([[15, 16], [17, 18]])\n",
    "\n",
    "np.concatenate((a, b, c), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set up the multiple (three) parallel phases as input for the LSTM model \n",
    "# Doc: https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "# For example, TRAIN_INDICES has train indices for each phase; train_arrays has values in each phase. \n",
    "# To merge them into LSTM's input, do the following steps:\n",
    "# 1. define these data as a matrix of 3 columns (phases) with n rows \n",
    "# 2. data = data.reshape(1, n, 3)\n",
    "# 3. Check the shape by print(data.shape)\n",
    "\n",
    "# train shape = (172, 3, 230, 230)\n",
    "lstm_input = np.zeros((172, 3))\n",
    "\n",
    "phase1_full_path = data_path + 'phase1/pos/'\n",
    "phase2_full_path = data_path + 'phase2/pos/'\n",
    "phase3_full_path = data_path + 'phase3/pos/'\n",
    "\n",
    "def create_lstm_training_input(): \n",
    "    phase1_pos_files = sorted(os.listdir(phase1_full_path))\n",
    "    phase2_pos_files = sorted(os.listdir(phase2_full_path))\n",
    "    phase3_pos_files = sorted(os.listdir(phase3_full_path))\n",
    "    \n",
    "    i = 0 \n",
    "    for index in TRAIN_INDICES: \n",
    "        phase1_arr = np.load(phase1_full_path + phase1_pos_files[index])\n",
    "        print(phase1_arr.shape)\n",
    "        phase2_arr = np.load(phase2_full_path + phase2_pos_files[index])\n",
    "        print(phase2_arr.shape)\n",
    "        phase3_arr = np.load(phase3_full_path +phase3_pos_files[index])\n",
    "        print(phase3_arr.shape)\n",
    "\n",
    "        # TODO: ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
    "        # For example: for P100.npy, (3, 230, 230), (3, 197, 174), (3, 199, 174)\n",
    "        lstm_input[i] = np.concatenate((phase1_arr, phase2_arr, phase3_arr), axis=1)\n",
    "        i += 1 \n",
    "   \n",
    "    # reshape to (1, ... )\n",
    "    lstm_training_input = lstm_input.reshape(1, n_train, 3) \n",
    "    \n",
    "    return lstm_training_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 230, 230)\n",
      "(3, 197, 174)\n",
      "(3, 199, 174)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-47624f2463d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_training_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lstm_training_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-0ef0c1a923b7>\u001b[0m in \u001b[0;36mcreate_lstm_training_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# TODO: ValueError: all the input array dimensions except for the concatenation axis must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# For example: for P100.npy, (3, 230, 230), (3, 197, 174), (3, 199, 174)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlstm_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase1_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase2_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase3_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "lstm_training_input = create_lstm_training_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, BatchNormalization, Dense, Flatten, Embedding\n",
    "from keras.layers.recurrent import RNN, LSTM \n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bb26609819c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(n_train, 3)))\n",
    "model.add(Dense(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
