{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: Separate out new training/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are trying to create a new directory of MIPs, the only three cells you should edit are: the cell directly below this description, and the last and second last cells. The cell directly below specifies a list of ID's which correspond to the cases you want/want to exclude. The last and second last cells specify paths to new directories on Google Cloud and gpu1708, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id's of all ALL 150 test cases\n",
    "# this includes the 52 scans from the greyscale paper as well as 98 additional scans\n",
    "# this dataset should be balanced (i.e. 75 pos and 75 neg)\n",
    "new_data = (\"04IOS24JP70LHBGB\",\n",
    "\"2A617MJ3LGPCXRTK\",\n",
    "\"2WN4Y1MAS89PSM9P\",\n",
    "\"4EH0PS1IKFIGT1T0\",\n",
    "\"4R0C9IQ512KYCGPC\",\n",
    "\"580WPPT613GFF945\",\n",
    "\"6BMRRS9RAZUPR3IL\",\n",
    "\"7GRXTJUB2643Z413\",\n",
    "\"ADIO4U6QJTXF3YQH\",\n",
    "\"C02TH0J1FT7XQ5EU\",\n",
    "\"C5ZH8SM0890PU4DL\",\n",
    "\"CGQFS5PB627OWLL7\",\n",
    "\"CWWKNDYG2UERXH7X\",\n",
    "\"DNO6CS3YNGFMUWXL\",\n",
    "\"DXOU8FRR7I9BLBN9\",\n",
    "\"F6MLW8PSZ0Z431D2\",\n",
    "\"FQJ4RD45AQ53MYRC\",\n",
    "\"GJ35FZQ5DSP09A4L\",\n",
    "\"GWGPR9W0KUGO4G6M\",\n",
    "\"H4C8M2RF0ASI8M5R\",\n",
    "\"HW93K0TDEQO9XO88\",\n",
    "\"IIOOPK6T2ZWDZMU3\",\n",
    "\"IWU76ETUE9UIZN8O\",\n",
    "\"IXRSXXZI0S6L0EJI\",\n",
    "\"JD6C377DW9DEXDDY\",\n",
    "\"K5584RWAW774MYAM\",\n",
    "\"KHPDR2WKRBY6ETGF\",\n",
    "\"KOQ6SDQIBEZAOCFQ\",\n",
    "\"LZU7NSTDRFX77U7G\",\n",
    "\"MD1ZI5BSP1QU1CUR\",\n",
    "\"MTWW42SPCGLHEDKY\",\n",
    "\"N0X30CNAU7RBG5UY\",\n",
    "\"NCU63Q4O8N50O6ND\",\n",
    "\"NTFYWO8XKLEW332D\",\n",
    "\"POSX6PS6LR34HENB\",\n",
    "\"QI8CI3NDYYJKGP9M\",\n",
    "\"QLYPHVVU3RG43OZO\",\n",
    "\"ROMHUDLTCOEXUQCA\",\n",
    "\"SUOA7XPFTTR2O5NQ\",\n",
    "\"THOT71FJ6S8OS792\",\n",
    "\"TVCP433TKOVBCQTW\",\n",
    "\"U82WU8SEKUCXCHZI\",\n",
    "\"UGXVSPJLHJL6AHSW\",\n",
    "\"UI0XEE9RCAEFSYP2\",\n",
    "\"WHDZVHCOCOTIJLXV\",\n",
    "\"WZPADD37RLXRX3IG\",\n",
    "\"XPJH44FSBRVL2WTV\",\n",
    "\"YEP4A7J1PJN5BMDO\",\n",
    "\"Z1GHH2QNG3BGMP4J\",\n",
    "\"VUTDS6OC2VUSNQMK\",\n",
    "\"G06KIRHKXWJZV4JL\",\n",
    "\"OAC7JXARELEVVEQW\",\n",
    "\"UBDRB3DMSB6TJ76N\",\n",
    "\"QKBS566N24RMCV5H\",\n",
    "\"LQG4BRZPSCMR5XAN\",\n",
    "\"IXBFNU6QB2SEZQ7N\",\n",
    "\"KFA8Q947Z1D0OAKW\",\n",
    "\"WSFKIA6H75FIIRF6\",\n",
    "\"FBJFYBF7A46A4GIM\",\n",
    "\"PAIDKQGSUI0KZ54S\",\n",
    "\"ZFMUMZVL6HFQCKSX\",\n",
    "\"8LWUQKCNESCFMX7J\",\n",
    "\"5V7B1QEMQF9IANBO\",\n",
    "\"JCWGEJIJO2LDRY2U\",\n",
    "\"GB9QW6WTB9XYMMN4\",\n",
    "\"OLCBKEY2WKCWPHJW\",\n",
    "\"YSU22E0MX5CRK3XC\",\n",
    "\"ROF7JT3SZGBNY3MG\",\n",
    "\"NLV36DRLRZY5AKIJ\",\n",
    "\"UKOHM6O57N37U2E2\",\n",
    "\"MSS82V0V6T9P13CV\",\n",
    "\"FVG8FNQ876DW8W9O\",\n",
    "\"PMEECXFHBLN6KHO7\",\n",
    "\"ABPO2BORDNF3OVL3\",\n",
    "\"6UI52CFMWHR6UH22\",\n",
    "\"CQHAXTV5WJLZBRFB\",\n",
    "\"EXX265Z1JC762YB7\",\n",
    "\"ZMP6NER3I4ERS2JE\",\n",
    "\"7EERK1STHKAHD0NH\",\n",
    "\"BGK2KBB8KXRWI65O\",\n",
    "\"LJRC1YH4ATELTZ7Y\",\n",
    "\"SRKOPGCEG62ZJTT2\",\n",
    "\"QHD8C9LSCKNYCRVH\",\n",
    "\"OXLMK9HYYWBECM3T\",\n",
    "\"JT6N68SSZWVYS4NF\",\n",
    "\"RFSGSDQ5MWIVM6QV\",\n",
    "\"54IOGT037BUFO9QN\",\n",
    "\"LDS0IN2DPOSUBAX4\",\n",
    "\"JOJU5LPDDBBP6LYG\",\n",
    "\"NYKZ3FK2LQ1Q5UP3\",\n",
    "\"NGQAJ2GBHZE2DN5C\",\n",
    "\"IUJ4I1BEYVZHV08V\",\n",
    "\"FA9ITR81JQB372QO\",\n",
    "\"L37U6THCBS65YAA9\",\n",
    "\"HDQYSVBJ4MPRGYRN\",\n",
    "\"C5A4R35NQOHJOCDZ\",\n",
    "\"1ZLUWMDXWGEAXY94\",\n",
    "\"SKBIT57CPAKAGQVB\",\n",
    "\"DDIKD2BEX8VMNNIQ\",\n",
    "\"0MTDDGCF20DKOR59\",\n",
    "\"OQNPNGEBR4D54PRA\",\n",
    "\"9UIAZ2U1711BN4IW\",\n",
    "\"HXLMZWH3SFX3SPAN\",\n",
    "\"IAUKV5R644JZFD55\",\n",
    "\"ILNTKMBVTXNXURGV\",\n",
    "\"JJMENP4QE4CSXSHV\",\n",
    "\"JWKB7SHIBYWSEVMC\",\n",
    "\"LGFNFIWO2ZEQYK36\",\n",
    "\"LINKQMUO9DQ43BNH\",\n",
    "\"LUVMEPI5JWYL67RF\",\n",
    "\"NHXCOHZ4HH53NLQ6\",\n",
    "\"RKBSU42WA7AY22E7\",\n",
    "\"RSKIY1U4X5QAUAAK\",\n",
    "\"SMGWMDYTYR8ZB3F5\",\n",
    "\"TRRYZ5WXYHUMTPCQ\",\n",
    "\"WWEFFBIMLZ3KLQVZ\",\n",
    "\"XSSFSN7XYAV4E3OA\",\n",
    "\"Z3AINLH4Y07ITBRR\",\n",
    "\"ZUEK5YSS7CITVWIP\",\n",
    "\"93BHW6GWBXMCXKZ\",\n",
    "\"W121FB8QX3LKCC5\",\n",
    "\"Y7TEI4R9A38060M\",\n",
    "\"J7OYZ0VL03RR0MG\",\n",
    "\"XM6CIDICOUPNBV9\",\n",
    "\"K138ZZNJBPCUJZU\",\n",
    "\"9PMFHK7Q3UGW0UM\",\n",
    "\"AZ23ZIFCYD9G1BN\",\n",
    "\"JJAET882O5AO7DD\",\n",
    "\"6WVZJW8VCKWPIGS\",\n",
    "\"WXRN2SWK763NZKT\",\n",
    "\"VIDM8DUZF5FBL5B\",\n",
    "\"8JTIFBJ85TJF7WZ\",\n",
    "\"E12QSRZG7DZDO3W\",\n",
    "\"8IG0XH3NO0SED7G\",\n",
    "\"FY5Y1WZYUR9OM85\",\n",
    "\"D5TB0LC9Q6ERYXP\",\n",
    "\"PZ3PAP7C8T6YVV9\",\n",
    "\"E2QR05XTWSSCB7T\",\n",
    "\"H0MSX6TSVLWG2LW\",\n",
    "\"0S14KBH0LQNHSIT\",\n",
    "\"WG2UEF2X6Q42ZJP\",\n",
    "\"450HUVNTNQY091H\",\n",
    "\"1TKIFTWTZB41S0N\",\n",
    "\"9SVLIU96EDGDX6W\",\n",
    "\"U0WYP8EFJLQG305\",\n",
    "\"MYSQVGRGT0FMC3E\",\n",
    "\"FHXDCGURXJBXD2F\",\n",
    "\"J8K098OLZFZH6LW\",\n",
    "\"2VEFNSOJ0IHTDQL\",\n",
    "\"CRLLJI0HYKR44H9\")\n",
    "\n",
    "# id of 52 cases from the greyscale scan\n",
    "# new_data = ('9UIAZ2U1711BN4IW',\n",
    "#             'HXLMZWH3SFX3SPAN',\n",
    "#             'IAUKV5R644JZFD55',\n",
    "#             'ILNTKMBVTXNXURGV',\n",
    "#             'JJMENP4QE4CSXSHV',\n",
    "#             'JWKB7SHIBYWSEVMC',\n",
    "#             'KOE9CU24WK2TUQ43',\n",
    "#             'LGFNFIWO2ZEQYK36',\n",
    "#             'LINKQMUO9DQ43BNH',\n",
    "#             'LUVMEPI5JWYL67RF',\n",
    "#             'NHXCOHZ4HH53NLQ6',\n",
    "#             'RKBSU42WA7AY22E7',\n",
    "#             'RSKIY1U4X5QAUAAK',\n",
    "#             'SMGWMDYTYR8ZB3F5',\n",
    "#             'TRRYZ5WXYHUMTPCQ',\n",
    "#             'WWEFFBIMLZ3KLQVZ',\n",
    "#             'XSSFSN7XYAV4E3OA',\n",
    "#             'Z3AINLH4Y07ITBRR',\n",
    "#             'ZUEK5YSS7CITVWIP',      \n",
    "#             '93BHW6GWBXMCXKZ',\n",
    "#             'W121FB8QX3LKCC5',\n",
    "#             'SPX5RHIM3I9G59H',\n",
    "#             'Y7TEI4R9A38060M',\n",
    "#             'CRLLJI0HYKR44H9',\n",
    "#             'J7OYZ0VL03RR0MG',\n",
    "#             'XM6CIDICOUPNBV9',\n",
    "#             'K138ZZNJBPCUJZU',\n",
    "#             '9PMFHK7Q3UGW0UM',\n",
    "#             'AZ23ZIFCYD9G1BN',\n",
    "#             'JJAET882O5AO7DD',\n",
    "#             '6WVZJW8VCKWPIGS',\n",
    "#             'WXRN2SWK763NZKT',\n",
    "#             'VIDM8DUZF5FBL5B',\n",
    "#             '8JTIFBJ85TJF7WZ',\n",
    "#             'E12QSRZG7DZDO3W',\n",
    "#             '8IG0XH3NO0SED7G',\n",
    "#             'FY5Y1WZYUR9OM85',\n",
    "#             'D5TB0LC9Q6ERYXP',\n",
    "#             'PZ3PAP7C8T6YVV9',\n",
    "#             'E2QR05XTWSSCB7T',\n",
    "#             'H0MSX6TSVLWG2LW',\n",
    "#             '0S14KBH0LQNHSIT',\n",
    "#             'WG2UEF2X6Q42ZJP',\n",
    "#             '450HUVNTNQY091H',\n",
    "#             '1TKIFTWTZB41S0N',\n",
    "#             '9SVLIU96EDGDX6W',\n",
    "#             'U0WYP8EFJLQG305',\n",
    "#             'MYSQVGRGT0FMC3E',\n",
    "#             'FHXDCGURXJBXD2F',\n",
    "#             'J8K098OLZFZH6LW',\n",
    "#             '2VEFNSOJ0IHTDQL',\n",
    "#             'UBS0O3YFB5ORS31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing purposes\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load bluenop.py\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from blueno.transforms import bound_pixels, crop\n",
    "\n",
    "# loading functions\n",
    "def load_arrays(data_dir: str) -> typing.Dict[str, np.ndarray]:\n",
    "    data_dict = {}\n",
    "    for filename in os.listdir(data_dir):\n",
    "        print(f'Loading file {filename}')\n",
    "        patient_id = filename[:-4]  # remove .npy extension\n",
    "        data_dict[patient_id] = np.load(pathlib.Path(data_dir) / filename)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def load_compressed_arrays(data_dir: str) -> typing.Dict[str, np.ndarray]:\n",
    "    data = dict()\n",
    "    for filename in os.listdir(data_dir):\n",
    "        print(f'Loading file {filename}')\n",
    "        d = np.load(pathlib.Path(data_dir) / filename)\n",
    "        data.update(d)  # merge all_data with d\n",
    "    return data\n",
    "\n",
    "# directly load in a csv file of new labels, pulled from github\n",
    "def load_new_labels(file_path: str) -> pd.DataFrame:\n",
    "    file_df: pd.DataFrame = pd.read_csv(file_path,\n",
    "        index_col='Anon ID')\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "def _zoom_array(arr: np.ndarray):\n",
    "    \"\"\"\"\n",
    "    if the image is too small to be cropped, zoom the image;\n",
    "    returns the zoomed image\n",
    "    \"\"\"\n",
    "    dims = arr.shape\n",
    "    second_dim = dims[1]\n",
    "    third_dim = dims[2]\n",
    "    return zoom(arr, (1, 220/second_dim, 220/third_dim))\n",
    "\n",
    "# processing functions\n",
    "def _process_array(arr: np.ndarray,\n",
    "                   preconfig: str):\n",
    "    # TODO(#64): Replace preconfig with parameters\n",
    "    if preconfig == 'standard-crop-mip':\n",
    "        arr = crop(arr,\n",
    "                   (75, 200, 200),\n",
    "                   height_offset=30)\n",
    "        arr = np.stack([arr[:25], arr[25:50], arr[50:75]])\n",
    "        arr = bound_pixels(arr, -40, 400)\n",
    "        arr = arr.max(axis=1)\n",
    "        arr = arr.transpose((1, 2, 0))\n",
    "        assert arr.shape == (200, 200, 3)\n",
    "        return arr\n",
    "    if preconfig == '220-crop-mip':\n",
    "        arr = crop(arr,\n",
    "                   (75, 220, 220),\n",
    "                   height_offset=30)\n",
    "        arr = np.stack([arr[:25], arr[25:50], arr[50:75]])\n",
    "        arr = bound_pixels(arr, -40, 400)\n",
    "        arr = arr.max(axis=1)\n",
    "        arr = arr.transpose((1, 2, 0))\n",
    "        assert arr.shape == (220, 220, 3)\n",
    "        return arr\n",
    "    if preconfig == 'new-crop':\n",
    "        arr = crop(arr,\n",
    "                   (75, 220, 220),\n",
    "                   height_offset=30)\n",
    "        arr = np.stack([arr[:25], arr[25:50], arr[50:75]])\n",
    "        arr = bound_pixels(arr, -40, 400)\n",
    "        arr = arr.max(axis=1)\n",
    "        arr = arr.transpose((1, 2, 0))\n",
    "        assert arr.shape == (220, 220, 3)\n",
    "        return arr\n",
    "\n",
    "    if preconfig == 'lower-crop-mip':\n",
    "        # hacky code below to zoom images that are too small,\n",
    "        # so that cropping is possible\n",
    "        # TODO generalize into a function\n",
    "        if (arr.shape[1] < 220) or (arr.shape[2] < 220):\n",
    "            arr = _zoom_array(arr)\n",
    "        arr = crop(arr,\n",
    "                   (75, 220, 220),\n",
    "                   height_offset=40)\n",
    "        arr = np.stack([arr[:25], arr[25:50], arr[50:75]])\n",
    "        arr = bound_pixels(arr, -40, 400)\n",
    "        arr = arr.max(axis=1)\n",
    "        arr = arr.transpose((1, 2, 0))\n",
    "        assert arr.shape == (220, 220, 3)\n",
    "        return arr\n",
    "\n",
    "    # TODO(#65): Optimize height offset, MIP thickness,\n",
    "    # MIP overlap, (M)IP variations, bounding values, input shape\n",
    "    # Save different configs as different if-statement blocks.\n",
    "\n",
    "    raise ValueError(f'{preconfig} is not a valid preconfiguration')\n",
    "\n",
    "\n",
    "def _process_arrays(arrays: typing.Dict[str, np.ndarray],\n",
    "                    preconfig: str) -> typing.Dict[str, np.ndarray]:\n",
    "    processed = {}\n",
    "    for id_, arr in arrays.items():\n",
    "        try:\n",
    "            processed[id_] = _process_array(arr, preconfig)\n",
    "        except AssertionError:\n",
    "            print(f'patient id {id_} could not be processed,'\n",
    "                  f' has input shape {arr.shape}')\n",
    "    return processed\n",
    "\n",
    "\n",
    "def process_data(arrays, labels, preconfig):\n",
    "    print(f'Using config {preconfig}')\n",
    "    processed_arrays = _process_arrays(arrays, preconfig)\n",
    "    processed_labels = labels.loc[processed_arrays.keys()]  # Filter, if needed\n",
    "    assert len(processed_arrays) == len(processed_labels)\n",
    "    return processed_arrays, processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(arrays: typing.Dict[str, np.ndarray],\n",
    "               labels: pd.DataFrame) -> \\\n",
    "        typing.Tuple[typing.Dict[str, np.ndarray], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Handle duplicates in the dataframe and removes\n",
    "    missing labels/arrays.\n",
    "\n",
    "    The output dictionary and dataframe will have the same\n",
    "    length.\n",
    "\n",
    "    :param arrays:\n",
    "    :param labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    filtered_arrays = arrays.copy()\n",
    "    for patient_id in arrays:\n",
    "        if patient_id not in labels.index.values:\n",
    "            print(f'{patient_id} in arrays, but not in labels. Dropping')\n",
    "            del filtered_arrays[patient_id]\n",
    "\n",
    "    filtered_labels = labels.copy()\n",
    "    print('Removing duplicate ids in labels:',\n",
    "          filtered_labels[filtered_labels.index.duplicated()].index)\n",
    "    filtered_labels = filtered_labels[~filtered_labels.index.duplicated()]\n",
    "\n",
    "    for patient_id in filtered_labels.index.values:\n",
    "        if patient_id not in arrays:\n",
    "            print(f'{patient_id} in labels, but not in arrays. Dropping')\n",
    "            filtered_labels = filtered_labels.drop(index=patient_id)\n",
    "\n",
    "    assert len(filtered_arrays) == len(filtered_labels)\n",
    "    return filtered_arrays, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting and saving functions\n",
    "def plot_images(data: typing.Dict[str, np.ndarray],\n",
    "                labels: pd.DataFrame,\n",
    "                num_cols=5,\n",
    "                limit=20,\n",
    "                offset=0):\n",
    "    \"\"\"\n",
    "    Plots limit images in a single plot.\n",
    "\n",
    "    :param data:\n",
    "    :param labels:\n",
    "    :param num_cols:\n",
    "    :param limit: the number of images to plot\n",
    "    :param offset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Ceiling function of len(data) / num_cols\n",
    "    num_rows = (min(len(data), limit) + num_cols - 1) // num_cols\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i, patient_id in enumerate(data):\n",
    "        if i < offset:\n",
    "            continue\n",
    "        if i >= offset + limit:\n",
    "            break\n",
    "        plot_num = i - offset + 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, plot_num)\n",
    "        ax.set_title(f'patient: {patient_id[:4]}...')\n",
    "        label = ('positive' if labels.loc[patient_id]['occlusion_exists']\n",
    "                 else 'negative')\n",
    "        ax.set_xlabel(f'label: {label}')\n",
    "        plt.imshow(data[patient_id])\n",
    "    fig.tight_layout()\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "def save_plots(arrays, labels, dirpath: str):\n",
    "    os.mkdir(dirpath)\n",
    "    num_plots = (len(arrays) + 19) // 20\n",
    "    for i in range(num_plots):\n",
    "        print(f'saving plot number {i}')\n",
    "        plot_images(arrays, labels, 5, offset=20 * i)\n",
    "        plt.savefig(f'{dirpath}/{20 * i}-{20 * i + 19}')\n",
    "\n",
    "\n",
    "def save_data(arrays: typing.Dict[str, np.ndarray],\n",
    "              labels: pd.DataFrame,\n",
    "              dirpath: str,\n",
    "              with_plots=True):\n",
    "    \"\"\"\n",
    "    Saves the arrays and labels in the given dirpath.\n",
    "\n",
    "    :param arrays:\n",
    "    :param labels:\n",
    "    :param dirpath:\n",
    "    :param with_plots:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # noinspection PyTypeChecker\n",
    "    os.makedirs(pathlib.Path(dirpath) / 'arrays')\n",
    "    for id_, arr in arrays.items():\n",
    "        # check if array is in downsampled set\n",
    "        if id_ in labels.index.values:\n",
    "            print(f'saving {id_}')\n",
    "            # noinspection PyTypeChecker\n",
    "            np.save(pathlib.Path(dirpath) / 'arrays' / f'{id_}.npy', arr)\n",
    "    labels.to_csv(pathlib.Path(dirpath) / 'labels.csv')\n",
    "    plots_dir = str(pathlib.Path(dirpath) / 'plots')\n",
    "    if with_plots:\n",
    "        save_plots(arrays, labels, plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the new arrays with names that are in the tuple defined above\n",
    "# change 'key in new_data' to 'key not in new_data' if goal is to \n",
    "# create dataset EXCLUDING id's above\n",
    "def get_new_arrays(arrays: typing.Dict[str, np.ndarray]) -> typing.Dict[str, np.ndarray]:\n",
    "    new_arrays = {}\n",
    "    for key, value in arrays.items():\n",
    "        if key in new_data:\n",
    "            new_arrays[key] = value\n",
    "    return new_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# directly load in a csv file of new labels, pulled from github\n",
    "def load_new_labels(file_path: str) -> pd.DataFrame:\n",
    "    file_df: pd.DataFrame = pd.read_csv(file_path,\n",
    "        index_col='Anon ID')\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# taken out of main function so that this time-consuming process only has to be run once\n",
    "# must load into notebook because there isn't enough space to store all\n",
    "# raw npy's on gpu1708\n",
    "raw_arrays = load_compressed_arrays('/home/lzhu7/elvo-analysis/data/numpy_compressed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function: filters out raw arrays to keep only desired cases, cleans cases to ensure labels\n",
    "# and arrays match up exactly, MIPs them, then saves the arrays, labels, and plots\n",
    "def get_new(args: dict):\n",
    "    new_arrays = get_new_arrays(args['raw_arrs'])\n",
    "    \n",
    "    new_labels = load_new_labels(args['labels_dir'])\n",
    "    new_row = pd.Series({'occlusion_exists': 1}, name='CRLLJI0HYKR44H9')\n",
    "    new_labels = new_labels.append(new_row)\n",
    "    \n",
    "    cleaned_new_arrays, cleaned_new_labels = clean_data(new_arrays, new_labels)\n",
    "    processed_arrays, processed_labels = process_data(cleaned_new_arrays,\n",
    "                                                      cleaned_new_labels,\n",
    "                                                      args['process_variant'])\n",
    "    save_data(processed_arrays, processed_labels, args['new_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    arguments = {\n",
    "        'raw_arrs': raw_arrays,\n",
    "        # this csv contains the most current csv of labels for ALL cases\n",
    "        'labels_dir': '/home/lzhu7/elvo-analysis/data/annotations_combined.csv',\n",
    "        # change this path to when creating a new dataset\n",
    "        'new_dir': '/home/lzhu7/elvo-analysis/data/complete-testset/',\n",
    "        'process_variant': 'lower-crop-mip',\n",
    "    }\n",
    "    get_new(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change these two paths below to match directory name of new_dir above\n",
    "!gsutil rsync -r /home/lzhu7/elvo-analysis/data/complete-testset gs://elvos/processed/complete-testset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
