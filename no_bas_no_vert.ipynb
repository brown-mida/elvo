{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from google.cloud import storage\n",
    "from etl.lib import transforms\n",
    "\n",
    "BLACKLIST = ['LAUIHISOEZIM5ILF',\n",
    "             '2018050121043822',\n",
    "             '2018050120260258',\n",
    "            ]\n",
    "\n",
    "\n",
    "class MipGenerator(object):\n",
    "\n",
    "    def __init__(self, dims=(120, 120, 1), batch_size=16,\n",
    "                 shuffle=True,\n",
    "                 validation=False,\n",
    "                 split=0.2, extend_dims=True,\n",
    "                 augment_data=True):\n",
    "        self.dims = dims\n",
    "        self.batch_size = batch_size\n",
    "        self.extend_dims = extend_dims\n",
    "        self.augment_data = augment_data\n",
    "        self.validation = validation\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "\n",
    "        # Delete all content in tmp/npy/\n",
    "        filelist = [f for f in os.listdir('tmp/npy')]\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join('tmp/npy', f))\n",
    "\n",
    "        # Get npy files from Google Cloud Storage\n",
    "        gcs_client = storage.Client.from_service_account_json(\n",
    "            'credentials/client_secret.json'\n",
    "        )\n",
    "        bucket = gcs_client.get_bucket('elvos')\n",
    "        blobs = bucket.list_blobs(prefix='multichannel_mip_data/from_numpy/')\n",
    "\n",
    "        files = []\n",
    "        for blob in blobs:\n",
    "            file = blob.name\n",
    "\n",
    "            # Check blacklist\n",
    "            blacklisted = False\n",
    "            for each in BLACKLIST:\n",
    "                if each in file:\n",
    "                    blacklisted = True\n",
    "\n",
    "            if not blacklisted:\n",
    "                # Add all data augmentation methods\n",
    "                files.append({\n",
    "                    \"name\": file,\n",
    "                })\n",
    "\n",
    "                if self.augment_data and not self.validation:\n",
    "                    self.__add_augmented(files, file)\n",
    "\n",
    "        # Split based on validation\n",
    "        if validation:\n",
    "            files = files[:int(len(files) * split)]\n",
    "        else:\n",
    "            files = files[int(len(files) * split):]\n",
    "\n",
    "        # Get label data from Google Cloud Storage\n",
    "        blob = storage.Blob('labels.csv', bucket)\n",
    "        blob.download_to_filename('tmp/labels.csv')\n",
    "        label_data = {}\n",
    "        with open('tmp/labels.csv', 'r') as pos_file:\n",
    "            reader = csv.reader(pos_file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] != 'patient_id':\n",
    "                    label_data[row[0]] = int(row[1])\n",
    "\n",
    "        labels = np.zeros(len(files))\n",
    "        for i, file in enumerate(files):\n",
    "            filename = file['name']\n",
    "            filename = filename.split('/')[-1]\n",
    "            filename = filename.split('.')[0]\n",
    "            filename = filename.split('_')[0]\n",
    "            labels[i] = label_data[filename]\n",
    "\n",
    "        # Take into account shuffling\n",
    "        if shuffle:\n",
    "            tmp = list(zip(files, labels))\n",
    "            random.shuffle(tmp)\n",
    "            files, labels = zip(*tmp)\n",
    "            labels = np.array(labels)\n",
    "\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.bucket = bucket\n",
    "\n",
    "    def __add_augmented(self, files, file):\n",
    "        for i in range(1):\n",
    "            files.append({\n",
    "                \"name\": file,\n",
    "            })\n",
    "\n",
    "    def generate(self):\n",
    "        steps = self.get_steps_per_epoch()\n",
    "        while True:\n",
    "            for i in range(steps):\n",
    "                print(i)\n",
    "                x, y = self.__data_generation(i)\n",
    "                yield x, y\n",
    "\n",
    "    def get_steps_per_epoch(self):\n",
    "        return len(self.files) // self.batch_size\n",
    "\n",
    "    def __data_generation(self, i):\n",
    "        bsz = self.batch_size\n",
    "        files = self.files[i * bsz:(i + 1) * bsz]\n",
    "        labels = self.labels[i * bsz:(i + 1) * bsz]\n",
    "        images = []\n",
    "\n",
    "        # Download files to tmp/npy/\n",
    "        for i, file in enumerate(files):\n",
    "            blob = self.bucket.get_blob(file['name'])\n",
    "            file_id = file['name'].split('/')[-1]\n",
    "            file_id = file_id.split('.')[0]\n",
    "            blob.download_to_filename(\n",
    "                'tmp/npy/{}.npy'.format(file_id)\n",
    "            )\n",
    "            img = np.load('tmp/npy/{}.npy'.format(file_id))\n",
    "            os.remove('tmp/npy/{}.npy'.format(file_id))\n",
    "            img = self.__transform_images(img)\n",
    "            # print(np.shape(img))\n",
    "            images.append(img)\n",
    "        images = np.array(images)\n",
    "        print(\"Loaded entire batch.\")\n",
    "        print(np.shape(images))\n",
    "        return images, labels\n",
    "\n",
    "    def __transform_images(self, image):\n",
    "        image = np.moveaxis(image, 0, -1)\n",
    "\n",
    "        # Set bounds\n",
    "        image[image < -40] = -40\n",
    "        image[image > 400] = 400\n",
    "\n",
    "        # Normalize image and expand dims\n",
    "        image = transforms.normalize(image)\n",
    "        if self.extend_dims:\n",
    "            if len(self.dims) == 2:\n",
    "                image = np.expand_dims(image, axis=-1)\n",
    "            else:\n",
    "                image = np.repeat(image[:, :, np.newaxis],\n",
    "                                  self.dims[2], axis=2)\n",
    "\n",
    "        # Data augmentation methods\n",
    "        if self.augment_data and not self.validation:\n",
    "            image = self.datagen.random_transform(image)\n",
    "\n",
    "        # Interpolate axis to reduce to specified dimensions\n",
    "        dims = np.shape(image)\n",
    "        image = zoom(image, (self.dims[0] / dims[0],\n",
    "                             self.dims[1] / dims[1],\n",
    "                             1))\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'sandbox/stage_1_resnet_v7_keep', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9b964284ba52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0mget_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mget_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9b964284ba52>\u001b[0m in \u001b[0;36mget_pred\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmodel_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sandbox/stage_1_resnet_v7_keep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     model_1 = Model(inputs=model_base.input,\n\u001b[1;32m     40\u001b[0m                     outputs=model_1(model_base.output))\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'sandbox/stage_1_resnet_v7_keep', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import keras.metrics as metrics\n",
    "from keras.layers import Average, Input\n",
    "from ml.generators.mip_generator import MipGenerator\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "\n",
    "metrics.sensitivity = sensitivity\n",
    "metrics.specificity = specificity\n",
    "\n",
    "\n",
    "def ensemble(models, model_input):\n",
    "    outputs = [model(model_input) for model in models]\n",
    "    y = Average()(outputs)\n",
    "    model = Model(model_input, y, name='ensemble')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_pred():\n",
    "    model_base = ResNet50(weights='imagenet', include_top=False)\n",
    "    model_1 = load_model('sandbox/stage_1_resnet_v7_keep')\n",
    "    model_1 = Model(inputs=model_base.input,\n",
    "                    outputs=model_1(model_base.output))\n",
    "    model_2 = load_model('sandbox/stage_1_resnet_v8_hold')\n",
    "    model_2 = Model(inputs=model_base.input,\n",
    "                    outputs=model_2(model_base.output))\n",
    "    model_3 = load_model('sandbox/stage_1_resnet_v9_keep')\n",
    "    model_3 = Model(inputs=model_base.input,\n",
    "                    outputs=model_3(model_base.output))\n",
    "\n",
    "    models = [model_1, model_2, model_3]\n",
    "    model_input = Input(shape=(220, 220, 3))\n",
    "    ensembleModel = ensemble(models, model_input)\n",
    "    ensembleModel.compile(optimizer='adam',\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "    gen = MipGenerator(\n",
    "        dims=(220, 220, 3),\n",
    "        batch_size=4,\n",
    "        augment_data=False,\n",
    "        extend_dims=False,\n",
    "        test=True,\n",
    "        split_test=True,\n",
    "        shuffle=True,\n",
    "        split=0.2\n",
    "    )\n",
    "\n",
    "    result = ensembleModel.evaluate_generator(\n",
    "        generator=gen.generate(),\n",
    "        steps=gen.get_steps_per_epoch(),\n",
    "        verbose=1\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "    result = ensembleModel.predict_generator(\n",
    "        generator=gen.generate(),\n",
    "        steps=gen.get_steps_per_epoch(),\n",
    "        verbose=1\n",
    "    ).ravel()\n",
    "\n",
    "    np.save('sandbox/pred.npy', result)\n",
    "\n",
    "\n",
    "def get_validation():\n",
    "    gen = MipGenerator(\n",
    "        dims=(220, 220, 3),\n",
    "        batch_size=4,\n",
    "        augment_data=False,\n",
    "        extend_dims=False,\n",
    "        test=True,\n",
    "        split_test=True,\n",
    "        shuffle=True,\n",
    "        split=0.2\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    generate = gen.generate()\n",
    "    print(gen.get_steps_per_epoch())\n",
    "    for i in range(gen.get_steps_per_epoch()):\n",
    "        data, labels = next(generate)\n",
    "        result.append(labels)\n",
    "    result = np.array(result).ravel()\n",
    "    np.save('sandbox/val.npy', result)\n",
    "\n",
    "\n",
    "def sensitivity_2(y_true, y_pred):\n",
    "    true_positives = np.sum(np.round(np.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = np.sum(np.round(np.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + 1e-07)\n",
    "\n",
    "\n",
    "def specificity_2(y_true, y_pred):\n",
    "    true_negatives = np.sum(np.round(np.clip((1 - y_true) *\n",
    "                                             (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = np.sum(np.round(np.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + 1e-07)\n",
    "\n",
    "\n",
    "def get_auc():\n",
    "    true_data = np.load('sandbox/val.npy')\n",
    "    pred_data = np.load('sandbox/pred.npy')\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(true_data, pred_data)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    print(sensitivity_2(true_data, pred_data))\n",
    "    print(specificity_2(true_data, pred_data))\n",
    "    target_names = ['No', 'Yes']\n",
    "    report = classification_report(true_data,\n",
    "                                   np.where(pred_data > 0.5, 1.0, 0.0),\n",
    "                                   target_names=target_names)\n",
    "    print(report)\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras,\n",
    "             label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "get_pred()\n",
    "get_validation()\n",
    "get_auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
